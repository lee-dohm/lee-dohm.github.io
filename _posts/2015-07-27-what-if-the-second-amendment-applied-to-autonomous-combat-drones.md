---
layout: link
title: What If the Second Amendment Applied to Autonomous Combat Drones?
address: http://futureoflife.org/AI/open_letter_autonomous_weapons
date: 2015-07-27 23:52:02.583562000 -07:00
---

I saw [this tweet from Elon Musk][musk-tweet] earlier tonight. It is in reference to an open letter by AI and robotics researchers stating that there should be a ban on artificial intelligence research for the purpose of creating offensive autonomous weapons. And the open letter got me to thinking ...

We've all seen what damage has been done by one person who has enough resources to afford a few hundred dollars worth of hand gun. Much of the time, they have more than just the one.[^1] And, [according to some research][spree-killers], about half of the "spree killers" that caused more than two casualties, killed innocent victims and then took their own life.

But what if an autonomous combat drone was available to such a person for perhaps the same price as two hand guns? Think of what damage could be done by someone who, rather than using a semi-automatic pistol, took an autonomous combat drone to a school, or a park, or a mall. And then, even worse than that ... they wouldn't even have to take this drone there by themselves. It could pilot itself. It could select targets on its own, perhaps even targeting them by race, age or gender.

Would the killer be guilt-ridden enough to kill themselves if they weren't even present to watch their victims die?[^2] A science fiction short story that had great impact on me as a young man made the point that humans have innate instincts that stop us from causing enough damage to someone to kill them ... at least until we invented weapons. Until we created weapons, like firearms, that allowed us to kill someone in an instant. Allowed us to kill someone before our instincts even had a chance to kick in. Weapons like cannons and howitzers and bombers that allowed us to kill someone before we could even *see* them.

You might say that these technologies would only be possessed by the military. But can we trust the military with such weapons? Can we trust the military to keep these technologies from falling into "the wrong hands"?[^3] We've seen how well that idea has protected the world from nuclear, biological and chemical weapons. Answer: not very well.

As was popularized in [a favorite movie of my youth][war-games], when the world was doing its best to fathom the **gigantic** hole it had dug for itself with the invention of the nuclear bomb, "the only winning move is not to play". The only real solution is to do our utmost to ensure that this technology *does not get invented*. That is why I, as a professional software developer, have signed on to this letter to call for a worldwide ban on the creation of offensive autonomous weapons.

The world doesn't need more efficient, less guilt-ridden, easier and cheaper ways to kill people.

[^1]: [A poll in 2004][ownership] found that 26% of individuals in the United States stated they owned at least one firearm. Almost half (48%) of those individuals reported owning *more than three* firearms.
[^2]: One theory about why "spree killers" take their own lives is to deny society its punishment of them. If they do not feel they will be caught, would they perhaps not kill themselves and then be able to kill again?
[^3]: Talking about technologies falling into "the wrong hands" of course presupposes that the military having them is "the right hands", which is **not** a given in my book.

[congress]: http://www.fas.org/sgp/crs/misc/RL32842.pdf
[musk-tweet]: https://twitter.com/elonmusk/status/625843527608459266
[ownership]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2610545/
[spree-killers]: http://www.wired.com/2012/12/why-spree-killers-kill-themselves/
[war-games]: http://www.imdb.com/title/tt0086567/?ref_=fn_al_tt_1
